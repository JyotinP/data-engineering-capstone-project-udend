{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Capstone_Proj_1') \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, TimestampType ,FloatType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Plan is to build an ETL pipeline which includes fetching data from various different sources which relates to I94 immigration data of US and then writing it to a desired format in parquet to make it easier for analytics team to perform analysis on it.\n",
    "- We'll be also using US city/state demographics data. This can be looked into by the analytics team so see if there's any correlation with the volume of people travelling to particular cities. \n",
    "- Tools used were Apache Spark, Pandas, Python. We want to use Spark as it distributes the workload over many different cluster nodes, so the data processing is much faster then compared to a tool like Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office. This is where the data comes from: https://www.trade.gov/national-travel-and-tourism-office\n",
    "\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft. https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "\n",
    "- I94_SAS_Labels_Descriptions.SAS: Provided by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_immigration = spark.read.parquet(r\"data/sas_data\")\n",
    "\n",
    "df_demographics = spark.read.options(delimiter=';').csv(r\"data/us-cities-demographics.csv\", header=True)\n",
    "\n",
    "country_label = spark.read.options(delimiter=';').csv(r\"data/I94_country_label.csv\", header=True)\n",
    "\n",
    "port_label = spark.read.options(delimiter=';').csv(r\"data/I94_port_label.csv\", header=True)\n",
    "\n",
    "mode_label = spark.read.options(delimiter=';').csv(r\"data/I94_mode_label.csv\", header=True)\n",
    "\n",
    "state_label = spark.read.options(delimiter=';').csv(r\"data/I94_state_label.csv\", header=True)\n",
    "\n",
    "visa_label = spark.read.options(delimiter=';').csv(r\"data/I94_visa_label.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "|         102|             ANDORRA|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|port_code|           port_name|\n",
      "+---------+--------------------+\n",
      "|      ALC|           ALCAN, AK|\n",
      "|      ANC|       ANCHORAGE, AK|\n",
      "|      BAR|BAKER AAF - BAKER...|\n",
      "|      DAC|   DALTONS CACHE, AK|\n",
      "|      PIZ|DEW STATION PT LA...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|mode_code|   mode_name|\n",
      "+---------+------------+\n",
      "|        1|         Air|\n",
      "|        2|         Sea|\n",
      "|        3|        Land|\n",
      "|        9|Not reported|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|state_code|state_name|\n",
      "+----------+----------+\n",
      "|        AL|   ALABAMA|\n",
      "|        AK|    ALASKA|\n",
      "|        AZ|   ARIZONA|\n",
      "|        AR|  ARKANSAS|\n",
      "|        CA|CALIFORNIA|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|visa_code|visa_name|\n",
      "+---------+---------+\n",
      "|        1| Business|\n",
      "|        2| Pleasure|\n",
      "|        3|  Student|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa_label.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "df_immigration = df_immigration.select('i94yr', 'i94mon', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa' \\\n",
    "                   ,'biryear', 'gender', 'airline', 'visatype')\n",
    "\n",
    "df_immigration = df_immigration.withColumnRenamed('i94yr', 'year') \\\n",
    "                    .withColumnRenamed('i94mon', 'month') \\\n",
    "                    .withColumnRenamed('i94res', 'ctry_of_res_code') \\\n",
    "                    .withColumnRenamed('i94port', 'port_code') \\\n",
    "                    .withColumnRenamed('arrdate', 'arrvl_dt_sas') \\\n",
    "                    .withColumnRenamed('i94mode', 'mode_code') \\\n",
    "                    .withColumnRenamed('i94addr', 'dest_state_code') \\\n",
    "                    .withColumnRenamed('depdate', 'depart_dt_sas') \\\n",
    "                    .withColumnRenamed('i94bir', 'age') \\\n",
    "                    .withColumnRenamed('i94visa', 'visa_code') \\\n",
    "                    .withColumnRenamed('biryear', 'dob') \\\n",
    "                    .withColumnRenamed('visatype', 'visa_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.filter(F.col(\"dest_state_code\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll filter out the records with missing state code\n",
    "\n",
    "df_immigration = df_immigration.filter(F.col(\"dest_state_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F|1242009|\n",
      "|  null| 392319|\n",
      "|     M|1308290|\n",
      "|     U|    325|\n",
      "|     X|    778|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_immigration.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll filter out the records with gender data\n",
    "\n",
    "df_immigration = df_immigration.filter(F.col(\"gender\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|mode_code|  count|\n",
      "+---------+-------+\n",
      "|      1.0|2485666|\n",
      "|      3.0|  47618|\n",
      "|      2.0|  11792|\n",
      "|      9.0|   6326|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Everything looks good here \n",
    "\n",
    "df_immigration.groupBy(\"mode_code\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|visa_code|  count|\n",
      "+---------+-------+\n",
      "|      1.0| 394817|\n",
      "|      3.0|  40238|\n",
      "|      2.0|2116347|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Everything looks good here \n",
    "\n",
    "df_immigration.groupBy(\"visa_code\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything looks good here , no nulls\n",
    "\n",
    "df_immigration.filter(F.col(\"ctry_of_res_code\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything looks good here, no nulls\n",
    "\n",
    "df_immigration.filter(F.col(\"port_code\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert SAS format date to readable format\n",
    "\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), int(arrvl_dt_sas)) AS arrvl_dt FROM immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert SAS format date to readable format\n",
    "\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table\")\n",
    "df_immigration = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), int(depart_dt_sas)) AS depart_dt FROM immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop SAS format date columns\n",
    "\n",
    "df_immigration = df_immigration.drop(\"arrvl_dt_sas\",\"depart_dt_sas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "|  year|month|ctry_of_res_code|port_code|mode_code|dest_state_code| age|visa_code|   dob|gender|airline|visa_type|  arrvl_dt| depart_dt|\n",
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             CA|40.0|      1.0|1976.0|     F|     QF|       B1|2016-04-30|2016-05-08|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             NV|32.0|      1.0|1984.0|     F|     VA|       B1|2016-04-30|2016-05-17|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|29.0|      1.0|1987.0|     M|     DL|       B1|2016-04-30|2016-05-08|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|29.0|      1.0|1987.0|     F|     DL|       B1|2016-04-30|2016-05-14|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|28.0|      1.0|1988.0|     M|     DL|       B1|2016-04-30|2016-05-14|\n",
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything looks good here , no nulls\n",
    "\n",
    "df_immigration.filter(F.col(\"arrvl_dt\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Creat dimension table date from the extracted values df_immigration\n",
    "\n",
    "dim_date = df_immigration.select(F.col(\"arrvl_dt\"),\n",
    "                            F.year(df_immigration[\"arrvl_dt\"]).alias(\"year\"),\n",
    "                            F.month(df_immigration[\"arrvl_dt\"]).alias(\"month\"),\n",
    "                            F.weekofyear(df_immigration[\"arrvl_dt\"]).alias(\"week_of_year\"),\n",
    "                            F.dayofyear(df_immigration[\"arrvl_dt\"]).alias(\"day_of_year\"),\n",
    "                            F.dayofmonth(df_immigration[\"arrvl_dt\"]).alias(\"day_of_month\"),\n",
    "                            F.dayofweek(df_immigration[\"arrvl_dt\"]).alias(\"day_of_week\"),\n",
    "                            F.quarter(df_immigration[\"arrvl_dt\"]).alias(\"quarter\")\n",
    "                           ).dropDuplicates([\"arrvl_dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+------------+-----------+------------+-----------+-------+\n",
      "|  arrvl_dt|year|month|week_of_year|day_of_year|day_of_month|day_of_week|quarter|\n",
      "+----------+----+-----+------------+-----------+------------+-----------+-------+\n",
      "|2016-04-25|2016|    4|          17|        116|          25|          2|      2|\n",
      "|2016-04-22|2016|    4|          16|        113|          22|          6|      2|\n",
      "|2016-04-30|2016|    4|          17|        121|          30|          7|      2|\n",
      "|2016-04-26|2016|    4|          17|        117|          26|          3|      2|\n",
      "|2016-04-04|2016|    4|          14|         95|           4|          2|      2|\n",
      "+----------+----+-----+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Aggregate the Dimension demographics table over the state code column\n",
    "\n",
    "dim_demographics = df_demographics.groupBy(df_demographics[\"State Code\"].alias(\"state_code\")).agg(\n",
    "    F.avg(\"Median Age\").cast(FloatType()).alias(\"medn_age\"),\n",
    "    F.sum(\"Male Population\").cast(IntegerType()).alias(\"male_pop\"),\n",
    "    F.sum(\"Female Population\").cast(IntegerType()).alias(\"female_pop\"),\n",
    "    F.sum(\"Total Population\").cast(IntegerType()).alias(\"total_pop\"),\n",
    "    F.sum(\"Number of Veterans\").cast(IntegerType()).alias(\"vet_cnt\"),\n",
    "    F.sum(\"Foreign-born\").cast(IntegerType()).alias(\"foreign_born\"),\n",
    "    F.avg(\"Average Household Size\").cast(FloatType()).alias(\"avg_hsehld_size\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+----------+---------+-------+------------+---------------+\n",
      "|state_code| medn_age|male_pop|female_pop|total_pop|vet_cnt|foreign_born|avg_hsehld_size|\n",
      "+----------+---------+--------+----------+---------+-------+------------+---------------+\n",
      "|        AZ|  35.0375|11137275|  11360435| 22497710|1322525|     3411565|       2.774375|\n",
      "|        SC|   33.825| 1265291|   1321685|  2586976| 163334|      134019|      2.4695833|\n",
      "|        LA|   34.625| 3134990|   3367985|  6502975| 348855|      417095|          2.465|\n",
      "|        MN|35.579628| 3478803|   3565362|  7044165| 321738|     1069888|       2.496852|\n",
      "|        NJ|35.254387| 3423033|   3507991|  6931024| 146632|     2327750|      2.9608772|\n",
      "+----------+---------+--------+----------+---------+-------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|state_code|count|\n",
      "+----------+-----+\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Everything looks good here. No duplicate values of state_code, \n",
    "# aggregation was performed successfully\n",
    "\n",
    "dim_demographics.groupBy(\"state_code\").count().where(\"count > 1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dimension table state\n",
    "\n",
    "dim_state = state_label.alias('dim_state')\n",
    "\n",
    "dim_state.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Dimension table country\n",
    "\n",
    "dim_country = country_label.alias('dim_country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change datatype of numeric code value to appropriate integer value\n",
    "dim_country = dim_country.withColumn(\"country_code\", F.col(\"country_code\").cast(IntegerType()))\n",
    "\n",
    "dim_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- port_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dimension table port\n",
    "\n",
    "dim_port = port_label.alias('dim_port')\n",
    "\n",
    "dim_port.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|mode_code|   mode_name|\n",
      "+---------+------------+\n",
      "|        1|         Air|\n",
      "|        2|         Sea|\n",
      "|        3|        Land|\n",
      "|        9|Not reported|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dimension table mode\n",
    "\n",
    "dim_mode = mode_label.alias('dim_mode')\n",
    "\n",
    "dim_mode.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mode_code: integer (nullable = true)\n",
      " |-- mode_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change datatype of numeric code value to appropriate integer value\n",
    "\n",
    "dim_mode = dim_mode.withColumn(\"mode_code\", F.col(\"mode_code\").cast(IntegerType()))\n",
    "\n",
    "dim_mode.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|visa_code|visa_name|\n",
      "+---------+---------+\n",
      "|        1| Business|\n",
      "|        2| Pleasure|\n",
      "|        3|  Student|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dimension table state\n",
    "\n",
    "dim_visa = visa_label.alias('dim_visa')\n",
    "\n",
    "dim_visa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visa_code: integer (nullable = true)\n",
      " |-- visa_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change datatype of numeric code value to appropriate integer value\n",
    "\n",
    "dim_visa = dim_visa.withColumn(\"visa_code\", F.col(\"visa_code\").cast(IntegerType()))\n",
    "\n",
    "dim_visa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "|  year|month|ctry_of_res_code|port_code|mode_code|dest_state_code| age|visa_code|   dob|gender|airline|visa_type|  arrvl_dt| depart_dt|\n",
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             CA|40.0|      1.0|1976.0|     F|     QF|       B1|2016-04-30|2016-05-08|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             NV|32.0|      1.0|1984.0|     F|     VA|       B1|2016-04-30|2016-05-17|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|29.0|      1.0|1987.0|     M|     DL|       B1|2016-04-30|2016-05-08|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|29.0|      1.0|1987.0|     F|     DL|       B1|2016-04-30|2016-05-14|\n",
      "|2016.0|  4.0|           438.0|      LOS|      1.0|             WA|28.0|      1.0|1988.0|     M|     DL|       B1|2016-04-30|2016-05-14|\n",
      "+------+-----+----------------+---------+---------+---------------+----+---------+------+------+-------+---------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Fact table immigration\n",
    "\n",
    "fact_immigration = df_immigration.alias('fact_immigration')\n",
    "\n",
    "fact_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- ctry_of_res_code: integer (nullable = true)\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- mode_code: integer (nullable = true)\n",
      " |-- dest_state_code: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_code: integer (nullable = true)\n",
      " |-- dob: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- arrvl_dt: date (nullable = true)\n",
      " |-- depart_dt: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = fact_immigration.withColumn(\"year\", F.col(\"year\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"month\", F.col(\"month\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"ctry_of_res_code\", F.col(\"ctry_of_res_code\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"mode_code\", F.col(\"mode_code\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"age\", F.col(\"age\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"visa_code\", F.col(\"visa_code\").cast(IntegerType())) \\\n",
    "                                    .withColumn(\"dob\", F.col(\"dob\").cast(IntegerType()))\n",
    "\n",
    "fact_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Conceptual Data Model is provided in the Readme.md file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Transform the 2 SAS date value columns 'arrvl_dt_sas' & 'depart_dt_sas' located in the immigrations table into standard readable date format.\n",
    "\n",
    "Create a new dimension table 'dim_date' by extracting various date related values from the 'arrvl_dt' located in the immigrations table.\n",
    "\n",
    "Transfrom the demographics table over the state code column to get demographics data per state.\n",
    "\n",
    "Write all Dimension table to Parquet files\n",
    "\n",
    "Write the Fact table to Parquet files.\n",
    "\n",
    "Clean the data wherever necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run the ETL pipeline which fetches data from source and writes to parquet\n",
    "\n",
    "%run -i etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Check Successful for fact_immigration\n",
      "Integrity Check Successfull for fact_immigration\n",
      "Count Check Successful for dim_date\n",
      "Integrity Check Successfull for dim_date\n",
      "Count Check Successful for dim_demographics\n",
      "Integrity Check Successfull for dim_demographics\n",
      "Count Check Successful for dim_country\n",
      "Integrity Check Successfull for dim_country\n",
      "Count Check Successful for dim_port\n",
      "Integrity Check Successfull for dim_port\n",
      "Count Check Successful for dim_mode\n",
      "Integrity Check Successfull for dim_mode\n",
      "Count Check Successful for dim_state\n",
      "Integrity Check Successfull for dim_state\n",
      "Count Check Successful for dim_visa\n",
      "Integrity Check Successfull for dim_visa\n"
     ]
    }
   ],
   "source": [
    "# Perform Data Quality checks to validate Counts and Integrity\n",
    "\n",
    "%run -i quality_checks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Dictionary is provided in the Readme.md file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#####     Answers\n",
    "\n",
    "- Apache Spark was chosen for this project since the source data is huge in size with records in millions. But since Spark distributes the workload over many different cluster nodes, the data processing is much faster then compared to a tool like Pandas.\n",
    "\n",
    "- Data can be updated daily since the immigration facts table is partitioned on year and month, so new data could be joined frequently.\n",
    "\n",
    "- If the data size was increased 100x then more nodes should be added in the cluster to increase the processing power.\n",
    "\n",
    "- Apache Airflow can be utilized to schedule daily job runs so data could be updated by 7 AM on the dashboard.\n",
    "\n",
    "- The data from parquet files could be copied over to Amazon Redshift which can handle data being accessed by 100+ people since it's a massive parallel   processing data warehouse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
